{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fA4bBUXqXubW"
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1745322606692,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "ek1vLyeiXudl",
    "outputId": "769cf800-6595-4081-8f1c-2d45564ff7b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1745322606721,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "nxOeTlBzXuf7",
    "outputId": "2e42fb35-998d-4a9c-88c6-862c936bc533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpVa8AJfY5cK"
   },
   "source": [
    "We are building character level model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwKG5HPsZHjX"
   },
   "source": [
    "Vocab Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745322606735,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "3l4L_A4YY470",
    "outputId": "fef8ba4a-0197-4e54-e078-a87aa8233c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pme1SUr7ZPwW"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue36SAvAZTKK"
   },
   "source": [
    "Very simple!, just using simple integers to map these characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1745322606908,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "7xjO51aiZAYm",
    "outputId": "d34ef15b-5f75-4791-94b3-851c27520dbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFp9LOf6apOP"
   },
   "source": [
    "We have build very simple tokenizer,\n",
    "\n",
    "Google uses SentencePiece (sub-word units)\n",
    "\n",
    "OpenAI uses tiktoken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHAoc0Uebrju"
   },
   "source": [
    "let's now encode the entire text dataset and store it into a torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5101,
     "status": "ok",
     "timestamp": 1745322612011,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "_q316D2Aaoug",
    "outputId": "c1a358b0-ed9f-4aff-967e-a5dfd100c87c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:10]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnVmcj0Xb8Yx"
   },
   "source": [
    "## Let's now split up the data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OGAnnbd-ZAdD"
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJk6ujJRccht"
   },
   "source": [
    "We are not going to feed all data at once, that would be computational expensive.\n",
    "\n",
    "Instead we going to train on chunk of data which we'll create from our original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745322612040,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "DxU5_7ksZAfa",
    "outputId": "caf21f49-e536-4910-d71b-c96ef8be0cc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTo0BZLaczqJ"
   },
   "source": [
    "In the block size of 8 (i.e, 9 characters), there are 8 examples individual packed in there.\n",
    "\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1745322612043,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "r9E0xL-IZAh6",
    "outputId": "445481b7-7589-4897-c38f-1f60656fcf3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhHCP3I6djK0"
   },
   "source": [
    "One reason to do this is that we want the transformer network to be used to see all the way from one to all the way up to block size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dobtY501eNmf"
   },
   "source": [
    "We are done with time dimension, now let's handle batch dimension. We feed the data into the network in mini-batch just to keep GPUs busy, as they are good at parallel processing. Also each example in batch doesn't talk to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1745322612089,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "-x4Nt8BidirX",
    "outputId": "19791997-128b-45a0-e521-2b964ed26297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74L60iEHfUkq"
   },
   "source": [
    "in input of 4 x 8 there are total of 32 examples packed in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMqDpiqbgmHf"
   },
   "source": [
    "First, let's build very simple bigram model using NN. We will have simple embedding weight and will train it and use it for prediction.\n",
    "\n",
    "Idea is very simple, we are not looking any context for prediction, we are saying every character has got something in it. By looking at a single character (that is itself), we can predict what follows next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "K0puDiy-ZAkM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "grpCvjG1ZAmy"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embd = 32 # added extra linear now\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        logits = self.lm_head(tok_emd) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745323882785,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "xn2LF8n9ZApP",
    "outputId": "6f965c34-cff8-4cef-d7f4-1ac50d401d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 32)\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n",
      "tensor(4.4922, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model.forward(xb, yb)\n",
    "\n",
    "print(model)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1745323884651,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "-QdOEfDSxoZx",
    "outputId": "eef85b6f-77a8-49c8-c529-7bd5f4930516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hYQRnbbmkMTUwbiu$?3KHvybsMEEFNLyb!SZgyGzRX$oNqTs!roUNLjMXM!EjT!hjmfH'ER3cOn.kvgAuau&e;m-CNLkfMW HT'R\n"
     ]
    }
   ],
   "source": [
    "# geneate from the model\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NhVA0kQ_vR2"
   },
   "source": [
    "# Adam vs AdamW?\n",
    "\n",
    "Adam:\n",
    "    Adam is an optimization algorithm that adapts the learning rate for each parameter. Itâ€™s an optimizer that combines:\n",
    "\n",
    "    Momentum: A running average of past gradients.\n",
    "    RMSprop: A running average of the squared gradients.\n",
    "\n",
    "## What's the Weight Decay Problem in Adam?\n",
    "\n",
    "ðŸš¨ Common practice before:\n",
    "\n",
    "People used to implement L2 regularization by modifying the gradient:\n",
    "\n",
    "```g_t += Î» * Î¸_t```\n",
    "\n",
    "This worked well with SGD, But Adam doesn't update directly with g_t â€” it scales the gradient with m_t, v_t etc.\n",
    "So, if you add weight decay to the gradient, it gets distorted\n",
    "\n",
    "This means:\n",
    "- Weight decay is now mixed with the adaptive scaling, which ruins the intended uniform shrinking effect.\n",
    "- So some weights shrink more, others less.\n",
    "- This leads to bad generalization, especially in large models (like Transformers).\n",
    "\n",
    "## How AdamW fixes this?\n",
    "\n",
    "The solution proposed in AdamW paper (2019) is:\n",
    "https://arxiv.org/abs/1711.05101\n",
    "\n",
    "**Decouple weight decay from the gradient update.**\n",
    "\n",
    "The weight decay step is independent of the gradient.\n",
    "\n",
    "It uniformly shrinks parameters by a fixed ratio:\n",
    "\n",
    "This keeps the regularization effect consistent, regardless of how gradients behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kdGPmt2TZAy9"
   },
   "outputs": [],
   "source": [
    "# let's train our model\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2895,
     "status": "ok",
     "timestamp": 1745323909737,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "wCuPiJ-1ZA1j",
    "outputId": "5e7a6a65-aa33-4e64-bd6e-0b2d5ac57703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5488028526306152\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for _ in range(1000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # zero all of the gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1745323911519,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "tDVLdcCMXukj",
    "outputId": "379489c9-6a22-4e1d-cfcd-3cc73373aed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tpour thea!\n",
      "She b.\n",
      "Worr;\n",
      "paet wh p'mou hof cod ive fondean hemen ossNCellld I beve ais\n",
      "Pice w; hevees hinean cofo atralorine, mhl t t and pninsy, yo I ves se\n",
      "Pind ays,ORhaipes t twaty e;\n",
      "Whan sirloworth?\n",
      "Mre, h ofise,\n",
      "CThed\n",
      "Whsplty;\n",
      "Ktor sear f yy bd :\n",
      "ABndou, beu\n",
      "\n",
      "\n",
      "AYer'd spatus soo me I,\n",
      "; cngoutl\n"
     ]
    }
   ],
   "source": [
    "# geneate from the model\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmlFHtnkFksg"
   },
   "source": [
    "Certainly not Shakespeare, but the model is making progress. This is the simplest model, because the tokens are not talking to each other.\n",
    "\n",
    "Now we want that token to talk to each other and figure out what's in context to make a better prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MK6kcWnqV5ii"
   },
   "source": [
    "Each token at time t should only \"see\" tokens from time â‰¤ t (no peeking into the future).\n",
    "\n",
    "A naive way: average all previous embeddings up to the current token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m041YguXCXG"
   },
   "source": [
    "# The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745322621646,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "bdf08VNKYhLB",
    "outputId": "373fbf11-7573-412d-c03c-f8473781b2a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oRER93glYn8v"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1745322621713,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "T4Cuy1kPXum3",
    "outputId": "dfd1b731-1e38-4c91-daec-ab8de4b94155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aOmotUQVD9Kt"
   },
   "outputs": [],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GvkNDMPCD9Pe"
   },
   "outputs": [],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgCPw4BQajDY"
   },
   "source": [
    "### Masking with Softmax (for attention weights)\n",
    "Instead of hard-coded ones in the lower triangle, allow learned attention weights.\n",
    "\n",
    "Initialize scores (e.g., all zeros), mask future with -inf, then apply softmax:\n",
    "\n",
    "# Why Softmax?\n",
    "Allows dynamic (data-dependent) attention:\n",
    "\n",
    "Instead of uniform averaging, future models will compute affinities between tokens.\n",
    "\n",
    "Attention scores dictate how much each token cares about previous ones.\n",
    "\n",
    "Future tokens are still masked to preserve causality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydnZfRYOc1Kh"
   },
   "source": [
    "Previously, we used a lower-triangular mask to perform uniform averaging over past tokens.\n",
    "\n",
    "That was a static mechanism: each token equally weighted previous ones\n",
    "\n",
    "# ðŸ§  Now the Goal:\n",
    "Make the weighting data-dependent â†’ certain tokens should attend more or less to others based on their content.\n",
    "\n",
    "# Core Idea: Queries, Keys, and Dot Products\n",
    "### 1. Every token emits 3 vectors:\n",
    "\n",
    "Query ð‘„: Q â†’ what I'm looking for\n",
    "\n",
    "Key ð¾: K â†’ what do I contain\n",
    "\n",
    "Value ð‘‰: V â†’ what I want to share\n",
    "\n",
    "Each of these is computed as a linear projection of the input:\n",
    "```\n",
    "k = self.key(x)    # B x T x head_size\n",
    "q = self.query(x)  # B x T x head_size\n",
    "v = self.value(x)  # B x T x head_size\n",
    "```\n",
    "\n",
    "### 2. Affinity Matrix (Weights):\n",
    "Dot product of each query with all keys\n",
    "\n",
    "```\n",
    "weights = q @ k.transpose(-2, -1)  # Shape: B x T x T\n",
    "```\n",
    "\n",
    "### 3. Causal Masking:\n",
    "Prevent future tokens from being seen (for language modeling):\n",
    "```\n",
    "weights.masked_fill(triu_mask == 0, float('-inf'))\n",
    "```\n",
    "\n",
    "### 4. Softmax Normalization:\n",
    "Convert affinities to probabilities:\n",
    "```\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "```\n",
    "\n",
    "### 5. Weighted Aggregation:\n",
    "Multiply attention weights by values:\n",
    "```\n",
    "output = weights @ v  # Shape: B x T x head_size\n",
    "```\n",
    "Each token aggregates information from previous tokens based on attention scores.\n",
    "\n",
    "Vectors v are what's actually communicatedâ€”not raw x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1745325528555,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "JsARtefTD9Rw",
    "outputId": "2b6eeb19-cfec-4263-cfa7-ea24dded8306"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WHzx31FjDWG"
   },
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uShDMlDkj6EY"
   },
   "source": [
    "Here are the expanded and structured notes for the final part of the explanation, incorporating your bullet points with clear elaboration, visual intuition, and technical depth:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Key Concepts Recap: Attention as Communication\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Attention as Communication\n",
    "- Think of each token as a **node in a directed graph**.\n",
    "- Each node:\n",
    "  - Holds its own vector (`x`)\n",
    "  - Computes a **weighted sum** of other nodes' values (`v`)\n",
    "  - Weights (affinities) are **data-dependent**, computed via dot product of `q` and `k`.\n",
    "\n",
    "> âš ï¸ Not fixed topologyâ€”data decides connectivity!\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§­ No Notion of Space\n",
    "- Attention is **permutation invariant** by default.\n",
    "- It treats all input vectors as a **set**, not a sequence or grid.\n",
    "- Thatâ€™s why we **must add position information** explicitly:\n",
    "  ```python\n",
    "  x = token_embedding + positional_embedding\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ’ Batch-Wise Isolation\n",
    "- Each input in the **batch dimension** is processed **independently**:\n",
    "  - No information is shared across batch entries.\n",
    "  - Each batch behaves like a **separate graph** of tokens.\n",
    "\n",
    "> Example: If batch size B = 4 and sequence length T = 8 â†’ We have **4 disjoint 8-node graphs**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Encoder vs Decoder Attention Blocks\n",
    "\n",
    "| Component       | Encoder Block                          | Decoder Block                          |\n",
    "|----------------|----------------------------------------|----------------------------------------|\n",
    "| Masking        | âŒ No masking â€“ full attention          | âœ… Masked â€“ only attends to past tokens |\n",
    "| Communication  | Bidirectional (all tokens see all)     | Autoregressive (future is masked)      |\n",
    "| Application    | BERT, T5 encoder                       | GPT, T5 decoder                        |\n",
    "| Masking Code   | *(none)*                               | `weights.masked_fill(triu == 0, -inf)` |\n",
    "\n",
    "- **Decoder block** masks future tokens using `torch.tril` (lower triangular mask).\n",
    "- **Encoder block** removes that line to allow all-to-all attention.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ Self-Attention vs Cross-Attention\n",
    "\n",
    "| Aspect         | Self-Attention                             | Cross-Attention                               |\n",
    "|----------------|--------------------------------------------|------------------------------------------------|\n",
    "| Queries        | Computed from input `x`                    | Computed from input `x`                        |\n",
    "| Keys & Values  | Also computed from same input `x`          | Come from a different source (e.g., encoder)   |\n",
    "| Application    | Language modeling, understanding           | Encoder-decoder models (e.g., translation)     |\n",
    "\n",
    "> ðŸ§© Cross-attention is what lets a decoder attend to encoder outputs in translation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Scaled Dot-Product Attention\n",
    "\n",
    "- Original affinity scores are computed as:\n",
    "  ```python\n",
    "  weights = q @ k.transpose(-2, -1)\n",
    "  ```\n",
    "\n",
    "- But as vector size increases, these dot products can grow large, leading to **softmax saturation** (only one element dominates).\n",
    "- Solution: **scale down the dot products**:\n",
    "  ```python\n",
    "  weights /= math.sqrt(head_size)\n",
    "  ```\n",
    "\n",
    "### ðŸ“ Why scaling helps:\n",
    "- Assume `q` and `k` have zero mean and unit variance.\n",
    "- Then `q Â· k` will have variance â‰ˆ `head_size`.\n",
    "- Dividing by `sqrt(head_size)` ensures:\n",
    "  - Dot product has **unit variance**\n",
    "  - Softmax stays **stable and diffuse**, improving training dynamics\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ˆ Illustration of Scaling Effect (conceptual)\n",
    "\n",
    "| Dot Product (Before Scaling) | After Scaling by âˆšd | Softmax Output |\n",
    "|------------------------------|---------------------|----------------|\n",
    "| [3.2, 2.1, 0.7]              | [0.8, 0.5, 0.2]     | [0.46, 0.35, 0.19] |\n",
    "| [10, 5, -3]                  | [2.5, 1.25, -0.75]  | [0.72, 0.23, 0.05] |\n",
    "\n",
    "- Without scaling: softmax becomes too **peaky**\n",
    "- With scaling: more **balanced**, less saturated gradients\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Final Summary\n",
    "\n",
    "| Concept                | Core Idea                                                                 |\n",
    "|------------------------|---------------------------------------------------------------------------|\n",
    "| Attention              | Communication between tokens via data-driven weighted aggregation         |\n",
    "| Queries & Keys         | Used to compute how \"interested\" tokens are in each other                 |\n",
    "| Values                 | Actual information that gets passed/aggregated                            |\n",
    "| Causal Masking         | Prevents tokens from accessing the future in language generation          |\n",
    "| Scaling                | Keeps softmax stable by normalizing dot product magnitudes                |\n",
    "| Self vs Cross Attention| Whether Q, K, V are from the same or different sources                    |\n",
    "| Encoder vs Decoder     | Whether all tokens talk, or only attend to previous tokens                |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to turn this into a visual flowchart or diagram next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPDA_Hs4lGKR"
   },
   "source": [
    "## ðŸ§® Scaled Dot-Product Attention\n",
    "\n",
    "### Why scale?\n",
    "- Prevent variance of attention weights from **blowing up** as the head size grows.\n",
    "- Without scaling:\n",
    "  - If Q, K are unit Gaussian, `QÂ·Káµ—` will have variance âˆ `head_size`.\n",
    "  - This can cause **softmax saturation** â†’ output becomes near one-hot.\n",
    "- With scaling:\n",
    "  ```python\n",
    "  scores = (Q @ K.transpose(-2, -1)) / sqrt(head_size)\n",
    "  ```\n",
    "\n",
    "### Effect on Softmax:\n",
    "- Keeps softmax output **diffuse** and prevents **gradient vanishing or explosion**.\n",
    "- At initialization, we want attention to be soft and spread, not overly focused.\n",
    "\n",
    "> **Takeaway**: Scaling by âˆšd is essential for **stable training** of attention models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ytdCY1SID9Uc"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1745326899838,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "M23w1uUAllXy",
    "outputId": "b0b0cd93-d1bc-4671-ba77-e819111af098"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1745326900334,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "COYptI87llaK",
    "outputId": "ff80f00f-ba06-4fd9-fda9-617dfd9c6731"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745326901118,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "jIzSKQb5llcf",
    "outputId": "28972c29-9eca-40d6-d456-58d68d9def5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745331006680,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "uWcB-4AWllfg",
    "outputId": "854ac89e-a4a3-4c6f-db4d-61b00545f628"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1745331013644,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "30Vamksh1a6C",
    "outputId": "c4c7da4c-1444-46d9-974b-0d4983b4b8bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvxb4naMlazJ"
   },
   "source": [
    "\n",
    "## ðŸ§± Implementation of Self-Attention Head\n",
    "\n",
    "### `Head` class (in PyTorch):\n",
    "- Takes `head_size` as input\n",
    "- Creates:\n",
    "  - Linear layers for `key`, `query`, `value` (no bias)\n",
    "  - Lower-triangular mask (`torch.tril`) registered as a buffer\n",
    "- Forward pass includes:\n",
    "  - Key, Query, Value projections\n",
    "  - Dot-product attention (with scaling)\n",
    "  - Masking (for decoder block behavior)\n",
    "  - Softmax + value aggregation\n",
    "\n",
    "## ðŸ”— Multi-Head Attention\n",
    "\n",
    "### Why multiple heads?\n",
    "- Allows **parallel, independent attention channels**.\n",
    "- Different heads can learn **different types of dependencies**:\n",
    "  - Some may focus on vowels, others on specific positions, etc.\n",
    "\n",
    "### Implementation:\n",
    "- Create multiple `Head` instances in a list:\n",
    "  ```python\n",
    "  self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "  ```\n",
    "- Forward:\n",
    "  ```python\n",
    "  x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "  ```\n",
    "\n",
    "### Head Size:\n",
    "- If `n_embed = 32` and `num_heads = 4`, then each head has `head_size = 8`.\n",
    "\n",
    "> Think of it like **grouped convolutions**, where each group processes different channels independently.\n",
    "\n",
    "### Training Outcome:\n",
    "- Validation loss improved: **2.4 â†’ 2.28**\n",
    "- Generation still rough, but better communication depth\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Feedforward Network (Per Token)\n",
    "\n",
    "### Why?\n",
    "- After tokens \"talk\" via attention, they should **process what theyâ€™ve heard**.\n",
    "- Add computation per token independently.\n",
    "\n",
    "### Implementation:\n",
    "```python\n",
    "self.ffwd = nn.Sequential(\n",
    "    nn.Linear(n_embed, n_embed),\n",
    "    nn.ReLU()\n",
    ")\n",
    "```\n",
    "\n",
    "- Applied **after self-attention**, token-wise:\n",
    "  ```python\n",
    "  x = self.head(x)\n",
    "  x = self.ffwd(x)\n",
    "  ```\n",
    "\n",
    "### Analogy:\n",
    "- Attention = group discussion  \n",
    "- Feedforward = individual thinking on what was heard\n",
    "\n",
    "### Effect:\n",
    "- Validation loss improved further: **2.28 â†’ 2.24**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§± Transformer Block Structure\n",
    "\n",
    "Each block:\n",
    "1. **Multi-head self-attention**\n",
    "2. **Feedforward network**\n",
    "3. Both are **per token**, and often include **residuals and layer norm** (to be added later)\n",
    "\n",
    "```text\n",
    "+-------------------+\n",
    "| Multi-head Attn   | â† Communication\n",
    "+-------------------+\n",
    "| Feedforward (MLP) | â† Computation\n",
    "+-------------------+\n",
    "```\n",
    "\n",
    "> These blocks are **stacked multiple times** in actual Transformer models (e.g., 12 for GPT-2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obFVsrnxw26a"
   },
   "source": [
    "# Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 253024,
     "status": "error",
     "timestamp": 1745331327262,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "IslnycL-jFzR",
    "outputId": "86cfe829-3ffb-4dac-b4e6-623c99ef772d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5091, val loss 2.5060\n",
      "step 300: train loss 2.4199, val loss 2.4338\n",
      "step 400: train loss 2.3499, val loss 2.3561\n",
      "step 500: train loss 2.2965, val loss 2.3129\n",
      "step 600: train loss 2.2405, val loss 2.2495\n",
      "step 700: train loss 2.2053, val loss 2.2194\n",
      "step 800: train loss 2.1625, val loss 2.1853\n",
      "step 900: train loss 2.1248, val loss 2.1513\n",
      "step 1000: train loss 2.1035, val loss 2.1305\n",
      "step 1100: train loss 2.0709, val loss 2.1193\n",
      "step 1200: train loss 2.0375, val loss 2.0789\n",
      "step 1300: train loss 2.0246, val loss 2.0646\n",
      "step 1400: train loss 1.9936, val loss 2.0371\n",
      "step 1500: train loss 1.9720, val loss 2.0318\n",
      "step 1600: train loss 1.9629, val loss 2.0479\n",
      "step 1700: train loss 1.9422, val loss 2.0139\n",
      "step 1800: train loss 1.9096, val loss 1.9963\n",
      "step 1900: train loss 1.9080, val loss 1.9884\n",
      "step 2000: train loss 1.8837, val loss 1.9943\n",
      "step 2100: train loss 1.8713, val loss 1.9749\n",
      "step 2200: train loss 1.8587, val loss 1.9602\n",
      "step 2300: train loss 1.8584, val loss 1.9554\n",
      "step 2400: train loss 1.8422, val loss 1.9460\n",
      "step 2500: train loss 1.8152, val loss 1.9416\n",
      "step 2600: train loss 1.8246, val loss 1.9368\n",
      "step 2700: train loss 1.8113, val loss 1.9327\n",
      "step 2800: train loss 1.8044, val loss 1.9223\n",
      "step 2900: train loss 1.8042, val loss 1.9304\n",
      "step 2999: train loss 1.8038, val loss 1.9243\n",
      "\n",
      "DUSCAM:\n",
      "Do hear, your is near for lack: of thenged they.\n",
      "That thy with strett and arle! this begity, lagget 't excolidges boott drue bitt stave ther?\n",
      "Aranclace, seld-begand-your bearn\n",
      "There for bear, a deed you cals'\n",
      "And this put macce is in but to will.\n",
      "\n",
      "QUEN VINIUS:\n",
      "I hereship you; frow will long tabird;\n",
      "But at in hat graverents pleasut aglaint,\n",
      "Sught her, How st cound I will the lay,\n",
      "I will my fuling?\n",
      "\n",
      "GLOUCESTEM:\n",
      "I sun would; then a face, dither, then all word; says,\n",
      "I'Tw see han gent and bood both,tagle at battened\n",
      "Ans with is fiar\n",
      "shat chont.\n",
      "\n",
      "CANILIUS:\n",
      "Tan yoush here.\n",
      "\n",
      "JUTUKISS:\n",
      "I his be bed; nigh iffives, I do hear\n",
      "Sent there botch, is passady a to to dift that do iffist these neavef-his that or acposile pocme, Now mostarre occuart thyshours,\n",
      "Are me should my cad be us: then I,? I so you bese a ritty thou findstribh.\n",
      "Where, and stake nid though again\n",
      "This wife peganst very, knoth\n",
      "Tund your hear-e senger bys'd, all I dem goods.\n",
      "\n",
      "QUEEL:\n",
      "In you shaught I dutay; srand\n",
      "The thy morow and out make slear chall,--mostile\n",
      "That bidspoked frow sin! you bost help him the arachilives\n",
      "for my dobty stand rence, thosguand moned as spett take,\n",
      "Let stmother flifed, in stirranc again crapessent, if this seak;\n",
      "Genst's new boys to me that out about\n",
      "areswatce concemy I paint hath of leatime.\n",
      "Theref the makes you lech but my in is came tuspost: wart; I but to your bust 'or is I will repar\n",
      "Hese sapurain ward myself?\n",
      "Which; you.\n",
      "And out why. Ack thm my Ran iffit;\n",
      "To amper and fatle littly till vence.\n",
      "\n",
      "VING LEONWARD:\n",
      "Let's is seep ald, which the with went;\n",
      "God my we sim a thou warry hil brock in rest you\n",
      "The coward'st make Bitte.\n",
      "Who Gwould me not they at arm is the poss stake that ow-hafflied\n",
      "bife dovery I were the nemes; but as and you shall with bed-nothse a most\n",
      "Did at fireight causter, sir, I that\n",
      "Bace of a a false heapod and fold say hatt a of abir my king\n",
      "That mother be ex-tnady him. Ne, caver, and shelf thy rest\n",
      "WI which as warmant is you is ought?\n",
      "Cobjent se heart 's thengue \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# # wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16725,
     "status": "ok",
     "timestamp": 1745331353058,
     "user": {
      "displayName": "Lucky Kushwaha",
      "userId": "03399689617171462108"
     },
     "user_tz": -330
    },
    "id": "faBrvU-ijF1f",
    "outputId": "79f1ab98-0db7-4201-b135-deba142f2020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I dace I tracius, the for of of an Xord abblences: by blolib'd it,\n",
      "I wave blouds thy shall the catt,\n",
      "Outy art of Bajoy block: piss; shat\n",
      "And struse!\n",
      "for mund for Jecemence bin-Seraker, wroth it eyetice;\n",
      "I my neat's That make him pit blest in gon you art,\n",
      "Setile all furse the and ver that bagling tilkand thos-loving;\n",
      "Adwe show to heave be berer gland eyes with his youUh:\n",
      "Shepp this as I will for a comestel but groust\n",
      "sill king to you sim. Then worthherd my ascome, at\n",
      "There boy's temppert bowh strall makin the spon.\n",
      "\n",
      "EXTMillow,\n",
      "BOLI mad it my ferer, and, not while her a scent.\n",
      "I what the never; be lest that. \n",
      "DUKE Strear: well be kill kings down?\n",
      "Brines Rome.\n",
      "\n",
      "RICHENTESTEM:\n",
      "O be you head! Let, goid nother, but boott, withse dimp you likery can of you lays lutumes of to const!\n",
      "Go not upong toth stume till brath\n",
      "As queepoty hang what me heart, of fail my kings!\n",
      "\n",
      "Cotherd Whath'd, a him takes of excoudit?\n",
      "\n",
      "LARENCE:\n",
      "Not mus'd mather, you his me; and blove in he,\n",
      "Let not be, I loved now, no where nep, wart with haspe that be\n",
      "plest our my stortunder, thank the heirg us bout I carmeo, me revery let\n",
      "O, she came tear ow bragge, I sir!\n",
      "\n",
      "CABELINT:\n",
      "Must hold, and sir,?\n",
      "\n",
      "KING HENRY VI hers.\n",
      "Shou of Julight, and her sy unwell his all wells\n",
      "And farce, pitting from! A hath the fair make look youns, Gives ritgrice,\n",
      "Marsart, prother, spord--then this blieth.\n",
      "\n",
      "KING EDICHASTINGSANE:\n",
      "I there battle, for go weater, I stis lifett; opent I' requeself jor take thus nave\n",
      "That a fion thee, my ginst yet rose;\n",
      "Wifich heave thy sen my objle sajot.\n",
      "Boy, the very miserd hords bull child\n",
      "I fair uppoass not desell Right.\n",
      "Cingle ropss their will ending welt hathy,\n",
      "I appincess or but suir dice nighmed the\n",
      "ous alverearw thy meath;\n",
      "That butisce min-mistroushmbeques,\n",
      "Soffales the will be you? morow them.\n",
      "Hell you well his broved sweet conful his my holy;\n",
      "There's hid nother dieaty in that fife,\n",
      "On nother Judiec and I from a king;\n",
      "Onc indince on yAuffightres art fout?\n",
      "\n",
      "MARGIO:\n",
      "Frawh lao? I good, affatheful' \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KDdI_4j21xd"
   },
   "source": [
    "## ðŸ”„ Embedding, Head Count, and Head Size\n",
    "\n",
    "- You define:\n",
    "  - `n_embed = 32`\n",
    "  - `n_head = 4`\n",
    "- This implies:\n",
    "  - `head_size = n_embed / n_head = 8`\n",
    "- Ensures that when you **concatenate** outputs from all heads, you get back to `n_embed`.\n",
    "\n",
    "> **Analogy**: Like group convolution â€” smaller communication blocks working in parallel.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Repeating Blocks (Communication + Computation)\n",
    "\n",
    "- A Transformer is built by **stacking blocks**:\n",
    "  - Each block = **Self-Attention** â†’ **Feedforward**\n",
    "- We now want to repeat:\n",
    "  ```python\n",
    "  Block\n",
    "  Block\n",
    "  Block\n",
    "  ...\n",
    "  ```\n",
    "\n",
    "> Deep networks improve model capacity, but also introduce optimization difficulties.\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Solution 1: Residual Connections (a.k.a. Skip Connections)\n",
    "\n",
    "- Origin: **ResNet** (2015)\n",
    "- Each block is wrapped with:\n",
    "  ```python\n",
    "  x = x + block(x)\n",
    "  ```\n",
    "- Helps in optimization:\n",
    "  - Enables **gradient flow** directly to early layers\n",
    "  - Prevents vanishing gradients\n",
    "  - At initialization, blocks do almost nothing â†’ clean gradient path\n",
    "- Visualization:\n",
    "  ```\n",
    "  Input\n",
    "    â”‚\n",
    "  â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚    Block      â”‚\n",
    "  â””â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚       â”‚\n",
    "    â””â”€â”€â”€ + â”€â”˜ â†’ Output\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Solution 2: Output Projection After Multi-Head Attention\n",
    "\n",
    "- After concatenating multi-head outputs, project back to `n_embed`:\n",
    "  ```python\n",
    "  self.proj = nn.Linear(n_embed, n_embed)\n",
    "  out = self.proj(concat_heads_output)\n",
    "  ```\n",
    "- Ensures shape compatibility with the **residual path**\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Scaling Feedforward Network Width\n",
    "\n",
    "- Paper (Vaswani et al., 2017) suggests:\n",
    "  - Inner dimension = 4 Ã— `n_embed`\n",
    "- Updated feedforward:\n",
    "  ```python\n",
    "  nn.Sequential(\n",
    "      nn.Linear(n_embed, 4 * n_embed),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4 * n_embed, n_embed)\n",
    "  )\n",
    "  ```\n",
    "\n",
    "> Adds expressive power while maintaining the same output shape.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Results after Residual + Projection + Scaling\n",
    "\n",
    "- Validation loss improves: **2.24 â†’ 2.08**\n",
    "- Slight overfitting starts appearing (train loss drops faster than val loss)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”ƒ Solution 3: Layer Normalization (LayerNorm)\n",
    "\n",
    "- Normalizes features **per token** (across embedding dim)\n",
    "- Very similar to BatchNorm, but:\n",
    "  - Does not depend on batch size\n",
    "  - No running stats needed\n",
    "  - No distinction between train/test mode\n",
    "\n",
    "### PyTorch:\n",
    "```python\n",
    "nn.LayerNorm(n_embed)\n",
    "```\n",
    "\n",
    "- Implemented **before** self-attention and feedforward (PreNorm)\n",
    "  ```python\n",
    "  x = x + sa(self.ln1(x))\n",
    "  x = x + ff(self.ln2(x))\n",
    "  ```\n",
    "\n",
    "> Normalizing before transformation improves training stability\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Updated Transformer Block Structure\n",
    "\n",
    "```\n",
    "Input â”€â”€â–º LayerNorm â”€â”€â–º Multi-Head Self Attention â”€â”€â–º + â”€â”€â–º LayerNorm â”€â”€â–º Feedforward â”€â”€â–º + â”€â”€â–º Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ›¡ï¸ Dropout for Regularization\n",
    "\n",
    "- Introduced Dropout (p=0.2) at several points:\n",
    "  - After self-attention before residual add\n",
    "  - After feedforward before residual add\n",
    "  - After softmax (drop connections randomly)\n",
    "\n",
    "### Why Dropout?\n",
    "- Randomly disables parts of the network â†’ prevents overfitting\n",
    "- Acts like an **ensemble** of subnetworks\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Scaling Up the Transformer\n",
    "\n",
    "### New Hyperparameters:\n",
    "```python\n",
    "batch_size = 64\n",
    "block_size = 256        # Context length\n",
    "n_embed = 384\n",
    "n_head = 6              # â†’ head_size = 64\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "```\n",
    "\n",
    "### Other Training Adjustments:\n",
    "- Learning rate lowered (larger network â†’ more sensitive)\n",
    "- Trained longer on A100 GPU (~15 min)\n",
    "\n",
    "### Results:\n",
    "- Validation loss improved from **2.07 â†’ 1.48**\n",
    "- Generated text resembles Shakespeare in style but remains semantically nonsensical\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ Decoder-Only Transformer\n",
    "\n",
    "### What's Missing?\n",
    "- **No Encoder Block**\n",
    "- **No Cross-Attention Block**\n",
    "\n",
    "### Why?\n",
    "- We're doing **unconditioned language modeling**\n",
    "- Just need to predict next token based on context (causal)\n",
    "\n",
    "### Masking:\n",
    "- Uses **lower-triangular masking** to enforce autoregressive property\n",
    "- No \"cheating\" by looking at the future\n",
    "\n",
    "> This is exactly what a **GPT-style decoder-only Transformer** does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkunAZti22gQ"
   },
   "source": [
    "Here are the **final detailed notes** wrapping up the full walkthrough of building a **decoder-only Transformer**, understanding how it's used in **GPT-style models**, and how it connects to **ChatGPT's training pipeline**. This ties everything togetherâ€”from raw self-attention to real-world deployment stages like fine-tuning and reinforcement learning.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Final Concepts and Wrap-up Notes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Decoder-Only Transformers\n",
    "\n",
    "### ðŸ”¹ What makes it a â€œDecoderâ€?\n",
    "- Uses a **triangular (causal) mask** in attention.\n",
    "- Enforces **auto-regressive generation**: each token only sees past tokens.\n",
    "- Suitable for **language modeling** â€” predicting the next token given previous context.\n",
    "\n",
    "### ðŸ”¹ When do you use a Decoder Only?\n",
    "- **Unconditional generation**: \"Just babble based on prior data.\"\n",
    "- No external context (e.g., translation source, prompt), just a corpus (like Shakespeare).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Encoder-Decoder Transformers\n",
    "\n",
    "### ðŸ”¹ Why do we need an encoder?\n",
    "- **Used in tasks like translation** where output must depend on external input.\n",
    "  - Input: Sentence in French\n",
    "  - Output: Sentence in English\n",
    "\n",
    "### ðŸ”¹ Architecture:\n",
    "\n",
    "1. **Encoder**:\n",
    "   - No masking (tokens can freely attend to each other).\n",
    "   - Converts input (e.g., French) into contextual embeddings.\n",
    "\n",
    "2. **Decoder**:\n",
    "   - Predicts output (e.g., English) **one token at a time**.\n",
    "   - Attends to:\n",
    "     - Its own **past tokens** (via causal mask)\n",
    "     - Full **encoder output** via **cross-attention**.\n",
    "\n",
    "### ðŸ”¹ Cross-Attention:\n",
    "- **Queries** from the decoder (current token)\n",
    "- **Keys and Values** from encoder outputs\n",
    "- Allows decoder to â€œlook atâ€ encoder context while generating\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Summary of What We Built\n",
    "\n",
    "| Module                 | Status             | Notes |\n",
    "|------------------------|--------------------|-------|\n",
    "| Embedding + Positional | âœ… Done             | Learnable embeddings |\n",
    "| Self-Attention         | âœ… With scaling, masking, dropout |\n",
    "| Multi-Head Attention   | âœ… Heads run in parallel |\n",
    "| Feedforward Layer      | âœ… Width = 4 Ã— embedding dim |\n",
    "| Residual Connections   | âœ… On attention & feedforward |\n",
    "| Layer Norm             | âœ… PreNorm style |\n",
    "| Dropout                | âœ… For regularization |\n",
    "| Stacked Blocks         | âœ… Parameterized by `n_layer` |\n",
    "| Final Layer Norm       | âœ… Before output projection |\n",
    "| Output Head            | âœ… Linear layer to vocabulary size |\n",
    "| Model Type             | âœ… Decoder-only GPT-style |\n",
    "\n",
    "> Architecture is essentially **identical to GPT models**, just much smaller.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Scaling Up: Hyperparameters (Final Large Model)\n",
    "\n",
    "| Hyperparameter     | Value       |\n",
    "|--------------------|-------------|\n",
    "| `n_layer`          | 6           |\n",
    "| `n_head`           | 6           |\n",
    "| `n_embed`          | 384         |\n",
    "| `block_size`       | 256         |\n",
    "| `dropout`          | 0.2         |\n",
    "| `batch_size`       | 64          |\n",
    "| `learning_rate`    | lower than earlier (due to deeper net) |\n",
    "\n",
    "### ðŸ”¹ Results:\n",
    "- Validation loss: **1.48** (down from 2.07)\n",
    "- Text generation mimics Shakespearean structure (though not semantically coherent yet)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Walkthrough (Codebase Overview)\n",
    "\n",
    "### ðŸ”¹ Two Key Files:\n",
    "- `train.py`: Training loop, checkpointing, learning rate scheduling, DDP support\n",
    "- `model.py`: Full Transformer model (similar to what we built)\n",
    "\n",
    "### ðŸ”¹ Differences:\n",
    "- Uses **batched multi-head attention** via 4D tensors\n",
    "- Built for **efficiency and scaling**\n",
    "- Uses GELU instead of ReLU in MLP (to match OpenAI's models)\n",
    "- Supports loading pretrained checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤– From GPT to ChatGPT: Whatâ€™s Different?\n",
    "\n",
    "### ðŸ“Œ GPT Pretraining:\n",
    "- Large **decoder-only Transformer**\n",
    "- Trained on internet-scale corpora (~300B tokens)\n",
    "- Goal: **document continuation**\n",
    "- Output: **unfiltered text completion**, not helpful Q&A\n",
    "\n",
    "### ðŸ“Œ ChatGPT Alignment Pipeline:\n",
    "\n",
    "1. **Supervised Fine-Tuning (SFT)**:\n",
    "   - Train on Q&A pairs (assistant-style formatting)\n",
    "   - Teaches the model to start behaving like a helper\n",
    "\n",
    "2. **Reward Model (RM) Training**:\n",
    "   - Human raters **rank multiple completions**\n",
    "   - Train a new model to **score responses**\n",
    "\n",
    "3. **Reinforcement Learning (PPO)**:\n",
    "   - Generate responses\n",
    "   - Use RM to give reward\n",
    "   - Tune the base model to maximize reward â†’ better, safer responses\n",
    "\n",
    "> The final ChatGPT pipeline is:\n",
    "> ```\n",
    "> GPT (Pretrained) â†’ SFT â†’ Reward Model â†’ PPO â†’ ChatGPT\n",
    "> ```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ Our Tiny GPT vs GPT-3\n",
    "\n",
    "| Attribute              | Our Model       | GPT-3             |\n",
    "|------------------------|-----------------|-------------------|\n",
    "| Parameters             | ~10M            | 175B              |\n",
    "| Tokens Trained On      | ~300K           | 300B              |\n",
    "| Vocab Type             | Character-level | Subword BPE (50K) |\n",
    "| Architecture           | GPT-style       | GPT-style         |\n",
    "| Training Time          | ~15 mins (A100) | Weeks (Thousands of GPUs) |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMcC+KirEVzTRYQkTfrId//",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
